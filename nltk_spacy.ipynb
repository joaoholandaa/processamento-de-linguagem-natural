{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###NLTK - Corpora"
      ],
      "metadata": {
        "id": "8QC4jfM9K7rm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3x8tOZSHJ1O",
        "outputId": "5bb78c46-d612-43b8-ace0-46b66e98b1ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/floresta.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk #Importando a biblioteca NLTK\n",
        "nltk.download('floresta') #Fazendo o download do corpus caso esse não tenha sido previamente baixado\n",
        "nltk.download('punkt') #Biblioteca utilizada para tokenizar as sentenças\n",
        "\n",
        "from nltk.corpus import floresta #Importando o corpus floresta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "floresta.fileids()[:3] #A função fileids() retorna os ids de todos os documentos do corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaKfNL15HqjH",
        "outputId": "9670c946-6641-4ec6-b5e4-51570afb614e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Floresta_7.4.ptb']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(floresta.raw('Floresta_7.4.ptb')[317:343]) #Retorna o conteúdo completo de um texto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua-mZHWgJuDC",
        "outputId": "8a38504a-2ee5-453a-9ec6-f686411dad9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Um revivalismo refrescante\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NLKT - Stopwords"
      ],
      "metadata": {
        "id": "nqz0me0QLGJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMYhuo4uNaSI",
        "outputId": "55a9e446-85c0-41fb-971b-47f59e8980b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Para retornar as stopwords de uma língua, basta utilizar\n",
        "a função words e informar a string da língua\n",
        "'''\n",
        "stopwords.words('portuguese')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG1y8z4iNbI5",
        "outputId": "6ee7656e-4f1f-4e00-c144-f1a4b360ef7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'à',\n",
              " 'ao',\n",
              " 'aos',\n",
              " 'aquela',\n",
              " 'aquelas',\n",
              " 'aquele',\n",
              " 'aqueles',\n",
              " 'aquilo',\n",
              " 'as',\n",
              " 'às',\n",
              " 'até',\n",
              " 'com',\n",
              " 'como',\n",
              " 'da',\n",
              " 'das',\n",
              " 'de',\n",
              " 'dela',\n",
              " 'delas',\n",
              " 'dele',\n",
              " 'deles',\n",
              " 'depois',\n",
              " 'do',\n",
              " 'dos',\n",
              " 'e',\n",
              " 'é',\n",
              " 'ela',\n",
              " 'elas',\n",
              " 'ele',\n",
              " 'eles',\n",
              " 'em',\n",
              " 'entre',\n",
              " 'era',\n",
              " 'eram',\n",
              " 'éramos',\n",
              " 'essa',\n",
              " 'essas',\n",
              " 'esse',\n",
              " 'esses',\n",
              " 'esta',\n",
              " 'está',\n",
              " 'estamos',\n",
              " 'estão',\n",
              " 'estar',\n",
              " 'estas',\n",
              " 'estava',\n",
              " 'estavam',\n",
              " 'estávamos',\n",
              " 'este',\n",
              " 'esteja',\n",
              " 'estejam',\n",
              " 'estejamos',\n",
              " 'estes',\n",
              " 'esteve',\n",
              " 'estive',\n",
              " 'estivemos',\n",
              " 'estiver',\n",
              " 'estivera',\n",
              " 'estiveram',\n",
              " 'estivéramos',\n",
              " 'estiverem',\n",
              " 'estivermos',\n",
              " 'estivesse',\n",
              " 'estivessem',\n",
              " 'estivéssemos',\n",
              " 'estou',\n",
              " 'eu',\n",
              " 'foi',\n",
              " 'fomos',\n",
              " 'for',\n",
              " 'fora',\n",
              " 'foram',\n",
              " 'fôramos',\n",
              " 'forem',\n",
              " 'formos',\n",
              " 'fosse',\n",
              " 'fossem',\n",
              " 'fôssemos',\n",
              " 'fui',\n",
              " 'há',\n",
              " 'haja',\n",
              " 'hajam',\n",
              " 'hajamos',\n",
              " 'hão',\n",
              " 'havemos',\n",
              " 'haver',\n",
              " 'hei',\n",
              " 'houve',\n",
              " 'houvemos',\n",
              " 'houver',\n",
              " 'houvera',\n",
              " 'houverá',\n",
              " 'houveram',\n",
              " 'houvéramos',\n",
              " 'houverão',\n",
              " 'houverei',\n",
              " 'houverem',\n",
              " 'houveremos',\n",
              " 'houveria',\n",
              " 'houveriam',\n",
              " 'houveríamos',\n",
              " 'houvermos',\n",
              " 'houvesse',\n",
              " 'houvessem',\n",
              " 'houvéssemos',\n",
              " 'isso',\n",
              " 'isto',\n",
              " 'já',\n",
              " 'lhe',\n",
              " 'lhes',\n",
              " 'mais',\n",
              " 'mas',\n",
              " 'me',\n",
              " 'mesmo',\n",
              " 'meu',\n",
              " 'meus',\n",
              " 'minha',\n",
              " 'minhas',\n",
              " 'muito',\n",
              " 'na',\n",
              " 'não',\n",
              " 'nas',\n",
              " 'nem',\n",
              " 'no',\n",
              " 'nos',\n",
              " 'nós',\n",
              " 'nossa',\n",
              " 'nossas',\n",
              " 'nosso',\n",
              " 'nossos',\n",
              " 'num',\n",
              " 'numa',\n",
              " 'o',\n",
              " 'os',\n",
              " 'ou',\n",
              " 'para',\n",
              " 'pela',\n",
              " 'pelas',\n",
              " 'pelo',\n",
              " 'pelos',\n",
              " 'por',\n",
              " 'qual',\n",
              " 'quando',\n",
              " 'que',\n",
              " 'quem',\n",
              " 'são',\n",
              " 'se',\n",
              " 'seja',\n",
              " 'sejam',\n",
              " 'sejamos',\n",
              " 'sem',\n",
              " 'ser',\n",
              " 'será',\n",
              " 'serão',\n",
              " 'serei',\n",
              " 'seremos',\n",
              " 'seria',\n",
              " 'seriam',\n",
              " 'seríamos',\n",
              " 'seu',\n",
              " 'seus',\n",
              " 'só',\n",
              " 'somos',\n",
              " 'sou',\n",
              " 'sua',\n",
              " 'suas',\n",
              " 'também',\n",
              " 'te',\n",
              " 'tem',\n",
              " 'tém',\n",
              " 'temos',\n",
              " 'tenha',\n",
              " 'tenham',\n",
              " 'tenhamos',\n",
              " 'tenho',\n",
              " 'terá',\n",
              " 'terão',\n",
              " 'terei',\n",
              " 'teremos',\n",
              " 'teria',\n",
              " 'teriam',\n",
              " 'teríamos',\n",
              " 'teu',\n",
              " 'teus',\n",
              " 'teve',\n",
              " 'tinha',\n",
              " 'tinham',\n",
              " 'tínhamos',\n",
              " 'tive',\n",
              " 'tivemos',\n",
              " 'tiver',\n",
              " 'tivera',\n",
              " 'tiveram',\n",
              " 'tivéramos',\n",
              " 'tiverem',\n",
              " 'tivermos',\n",
              " 'tivesse',\n",
              " 'tivessem',\n",
              " 'tivéssemos',\n",
              " 'tu',\n",
              " 'tua',\n",
              " 'tuas',\n",
              " 'um',\n",
              " 'uma',\n",
              " 'você',\n",
              " 'vocês',\n",
              " 'vos']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK - Tokenização"
      ],
      "metadata": {
        "id": "urmiiLLYOwdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize #Importando o tokenizador de palavras\n",
        "from nltk.tokenize import sent_tokenize #Importando o tokenizador de sentenças\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "MT6Hq9w-NcuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf93823-1614-4459-87f5-0f331a5ce891"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando um texto de exemplo\n",
        "texto = 'Goku is a hero in the Dragon Ball since 1989! Goku saves the earth so many times.'"
      ],
      "metadata": {
        "id": "RniA8PR6O4fn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.sent_tokenize(texto) #Tokenizando um texto considerando sentenças como unidades"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdE8VawJO5tR",
        "outputId": "2cc9492a-fed8-4eb4-d258-1771f7215d0d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Goku is a hero in the Dragon Ball since 1989!',\n",
              " 'Goku saves the earth so many times.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(texto) #Tokenizando um texto considerando palavras como unidades"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBIAKlYEO8rK",
        "outputId": "dc5c433e-0996-45ea-b564-d3cfcc149263"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Goku',\n",
              " 'is',\n",
              " 'a',\n",
              " 'hero',\n",
              " 'in',\n",
              " 'the',\n",
              " 'Dragon',\n",
              " 'Ball',\n",
              " 'since',\n",
              " '1989',\n",
              " '!',\n",
              " 'Goku',\n",
              " 'saves',\n",
              " 'the',\n",
              " 'earth',\n",
              " 'so',\n",
              " 'many',\n",
              " 'times',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "texto2 = \"I'm very veryyyy happyyyyy #betterlife @barneys :p :D\""
      ],
      "metadata": {
        "id": "-JATR5ICP7ri"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitterTokenizer = TweetTokenizer()\n",
        "twitterTokenizer.tokenize(texto2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek7yLU--P8VC",
        "outputId": "d1415422-933c-46b2-f13d-2828fc44e777"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\", 'very', 'veryyyy', 'happyyyyy', '#betterlife', '@barneys', ':p', ':D']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "-strip_handles=True: remove os nomes de usuário do texto\n",
        "-reduce_len=True: substitui sequencias de 3 ou mais caracteres repetidos por sequencia de 3 caracteres\n",
        "\"\"\"\n",
        "twitterTokenizer2 = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "twitterTokenizer2.tokenize(texto2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boTneywJP_eY",
        "outputId": "617aa657-e7b4-453d-e3ee-2371fd55794e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\", 'very', 'veryyy', 'happyyy', '#betterlife', ':p', ':D']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NLTK - Simplificação das Palavras"
      ],
      "metadata": {
        "id": "V7yN88AV-MDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "MLdSRN4_-Qck"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SnowballStemmer.languages #Listando as linguas suportadas pelo SnowBallStemmer\n",
        "snowballStemmer = SnowballStemmer(\"portuguese\") #Criando um objeto stemizador"
      ],
      "metadata": {
        "id": "DdAJ6uTx_8Y8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballStemmer.stem('computação') #Radicalizando as palavras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "amb7KAEI_-i1",
        "outputId": "3eca9da2-2282-4ebd-de46-db69c22f15a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comput'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballStemmer.stem('computador')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eaYyW9LxAASp",
        "outputId": "0ffee7c9-108f-4c19-d5c8-b155c7ecb7de"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comput'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballStemmer.stem('computando')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zChRm2BaADAe",
        "outputId": "ff393a26-7367-4d0c-d984-d83e44a52cec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comput'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer #Importando o WordNetLemmatizer e fazendo download do recurso caso não exista\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZk--pUsAD5y",
        "outputId": "80d9c716-88d4-403c-e6ba-d7a656928423"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Também é possível informar a função sintática da palavra no parâmetro 'pos' .\\\n",
        "\tNo exemplo abaixo, 'a' denota adjetivo\"\"\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize(\"better\", pos = \"a\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1RU7v2izAG0A",
        "outputId": "1bfd8654-a107-478a-b3e4-8be0817885b3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'good'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"O uso da função sintática pode alterar/melhorar o resultado da lematização\"\"\"\n",
        "lemmatizer.lemmatize(\"clustering\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mEVzGYakA11d",
        "outputId": "e984b52f-09a7-4b7d-dcc1-2b63e912253c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'clustering'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"clustering\", pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WBl_dNEfA430",
        "outputId": "cde8b6dc-42ff-47cd-ef54-8668f5e78c48"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cluster'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SPACY - Tokenização"
      ],
      "metadata": {
        "id": "cDlnXs2OBq4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\"\"\"Gerando um objeto o qual gerará um objeto iteravel e cada item representa um token, e\n",
        "   cada token contém além da palavra em sei, sua versão simplificada, função sintática, etc.\"\"\"\n",
        "\n",
        "sentenca = nlp('Rafael is working at Google in the South America. He works very hard :D')\n",
        "\n",
        "for token in sentenca:\n",
        "\tprint(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btHQ7N_kEfFg",
        "outputId": "fecfe3f5-7174-4c72-e322-9e33ed39c4a4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rafael\n",
            "is\n",
            "working\n",
            "at\n",
            "Google\n",
            "in\n",
            "the\n",
            "South\n",
            "America\n",
            ".\n",
            "He\n",
            "works\n",
            "very\n",
            "hard\n",
            ":D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Caso queira considerar primeiramente cada sentença como token também é possível\n",
        "#Para isso basta utilizar a propriedade sents do objeto gerado\n",
        "doc = nlp('Lucas is working at Google in the South America. He works very hard!')\n",
        "\n",
        "for sent in doc.sents:\n",
        "\tprint(sent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYVuEUugEmOe",
        "outputId": "f8afb4a5-e99d-4c58-c74c-9717b0ce0772"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lucas is working at Google in the South America.\n",
            "He works very hard!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SPACY - Stopwords"
      ],
      "metadata": {
        "id": "ZvGyE_vCEuBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentenca = nlp('Lucas is working at Google in the South America.\\\n",
        "\t\tHe works very hard!')\n",
        "\n",
        "for token in sentenca: #A propriedade .is_stop retorna True se o token está na lista de stopwords\n",
        "\tprint(f'{token.text} - {token.is_stop}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt5gFCgaEslp",
        "outputId": "b52e7cd4-11eb-4a13-d2e1-8856b8ecde23"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lucas - False\n",
            "is - True\n",
            "working - False\n",
            "at - True\n",
            "Google - False\n",
            "in - True\n",
            "the - True\n",
            "South - False\n",
            "America - False\n",
            ". - False\n",
            "\t\t - False\n",
            "He - True\n",
            "works - False\n",
            "very - True\n",
            "hard - False\n",
            "! - False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SPACY - Composição dos tokens"
      ],
      "metadata": {
        "id": "pdfo9cpnE1rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentenca = nlp('Lucas is working at Google in the South America since 1999.\\\n",
        "\t\t            He works very hard!')\n",
        "\n",
        "#A propriedade .is_alpha retorna True se o token é composto apenas por caracteres alfabéticos\n",
        "for token in sentenca:\n",
        "\tprint(f'{token.text} - {token.is_alpha}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7O8tlR3E4N8",
        "outputId": "b3e1d47d-58ee-4f0d-c976-234de03af32a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lucas - True\n",
            "is - True\n",
            "working - True\n",
            "at - True\n",
            "Google - True\n",
            "in - True\n",
            "the - True\n",
            "South - True\n",
            "America - True\n",
            "since - True\n",
            "1999 - False\n",
            ". - False\n",
            "\t\t             - False\n",
            "He - True\n",
            "works - True\n",
            "very - True\n",
            "hard - True\n",
            "! - False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###SPACY - Lematização"
      ],
      "metadata": {
        "id": "3-Oz6l5pFD3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentenca = nlp('Lucas has drinked two coffees while the\\\n",
        "\t\tcomputer is computing the values of the matrices.')\n",
        "\n",
        "for token in sentenca:\n",
        "\tprint(f'{token.text} - {token.lemma_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wjf6BJ3E_Us",
        "outputId": "00c7c442-82d7-43e6-e8ee-71f9449341c4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lucas - Lucas\n",
            "has - have\n",
            "drinked - drink\n",
            "two - two\n",
            "coffees - coffee\n",
            "while - while\n",
            "the - the\n",
            "\t\t - \t\t\n",
            "computer - computer\n",
            "is - be\n",
            "computing - compute\n",
            "the - the\n",
            "values - value\n",
            "of - of\n",
            "the - the\n",
            "matrices - matrix\n",
            ". - .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SPACY - POS Tagging"
      ],
      "metadata": {
        "id": "4_eWfWv6FKlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentenca = nlp('Rafael is working at Google in the South America. He works very hard!')\n",
        "\n",
        "for token in sentenca:\n",
        "\tprint(f'{token.text} - {token.pos_} - {token.tag_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZryMAgdFKQO",
        "outputId": "5c76980d-1083-45a5-f68d-b6a13aea7a6d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rafael - PROPN - NNP\n",
            "is - AUX - VBZ\n",
            "working - VERB - VBG\n",
            "at - ADP - IN\n",
            "Google - PROPN - NNP\n",
            "in - ADP - IN\n",
            "the - DET - DT\n",
            "South - PROPN - NNP\n",
            "America - PROPN - NNP\n",
            ". - PUNCT - .\n",
            "He - PRON - PRP\n",
            "works - VERB - VBZ\n",
            "very - ADV - RB\n",
            "hard - ADV - RB\n",
            "! - PUNCT - .\n"
          ]
        }
      ]
    }
  ]
}